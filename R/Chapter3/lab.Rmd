---
title: "ISLR Chapter3 "
author: "ACO"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: '6'
  html_document:
    df_print: paged
    toc: true
    toc_depth: '6'
urlcolor: blue
subtitle: 'Linear Regression: Labs'
---

\newpage
```{r, include=FALSE}
knitr::opts_chunk$set(echo = T, warning=FALSE,message=FALSE,attr.source='.numberLines',attr.output='.numberLines', fig.align='center', dpi=350, fig.width=15, fig.height=15)
```

# Linear Regression

## 3.6.1 Libraries

```{r}
library(MASS)
library(ISLR2)
```

## 3.6.2 Simple Linear Regression

Predict `medv` using 12 predictors such as `rm` (average number of rooms per house),
`age` (proportion of owner-occupied units built prior to 1940) and `lstat` (percent
of households with low socioeconomic status).

```{r}
str(Boston, 2)
```

```{r}
lm_fit <- lm(medv ~ lstat , data = Boston)
summary(lm_fit)
```

confidence interval for coeficient estimates

```{r}
confint(lm_fit)
```

confidence interval for predictions

```{r}
predict(
  lm_fit , data.frame(lstat = (c(5, 10, 15))), interval = "confidence")
predict(
  lm_fit , data.frame(lstat = (c(5, 10, 15))), interval = "prediction")
```

```{r}
plot(medv ~ lstat , data = Boston)
abline(lm(medv ~ lstat , data = Boston), col = 'blue')
```

```{r}
par(mfrow = c(2,2))
plot(lm(medv ~ lstat , data = Boston))
par(mfrow = c(1,1))
```

```{r}
par(mfrow = c(2,2))
plot(predict(lm_fit), residuals(lm_fit))
plot(predict(lm_fit), rstudent(lm_fit))
plot(hatvalues(lm_fit))
par(mfrow = c(1,1))
```

```{r}
which.max(hatvalues(lm_fit))
```

## 3.6.3 Multiple Linear Regression

```{r}
lm_fit <- lm(medv ~ lstat + age , data = Boston)
summary(lm_fit)
```

```{r}
lm_fit <- lm(medv ~ ., data = Boston)
summary(lm_fit)
```

Hence R-Squared = `r `summary(lm.fit)$r.sq` and RSE = `summary(lm.fit)$sigma`

```{r}
library(car)
```

```{r}
vif(lm_fit)
```

## 3.6.4 Interaction Terms

```{r}
# interaction, use : or *
lm_fit <- lm(medv ~ lstat:age , data = Boston)
summary(lm_fit)
```

## 3.6.5 Non-linear Transformations of the Predictors

```{r}
lm_fit <- lm(medv ~ lstat , data = Boston)
summary(lm_fit)
```

```{r}
lm_fit2 <- lm(medv ~ lstat + I(lstat^2) , data = Boston)
summary(lm_fit)
```

```{r}
anova(lm_fit, lm_fit2)
```


```{r, eval=F}
lm_fit <- lm(medv ~ lstat + poly(lstat, 2, raw = TRUE) , data = Boston)
summary(lm_fit)
```

## 3.6.6 Qualitative Predictors

```{r}
str(Carseats)
```

```{r}
lm_fit <- lm(
  Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(lm_fit)
```

```{r}
contrasts(Carseats$ShelveLoc)
```

\newpage

# Applied

## Q8

```{r}
q8_lm = lm(mpg~horsepower, data = Auto)
summary(q8_lm)
```

i. Yes
ii. `r cor(Auto$mpg, Auto$horsepower)`
iii. R squared 60% 
iv 
```{r}
predict(q8_lm, newdata = data.frame(horsepower = 98))
```

```{r}
predict(q8_lm, newdata = data.frame(horsepower = 98), interval = "confidence")
predict(q8_lm, newdata = data.frame(horsepower = 98), interval = "prediction")
```

```{r}
q8_lm = lm(mpg~horsepower, data = Auto)
plot(mpg~horsepower, data = Auto)
abline(q8_lm)
```

```{r}
par(mfrow = c(2,2))
plot(q8_lm)
par(mfrow = c(1,1))
```

\newpage
## Q9

### a

```{r}
pairs(Auto)
```

### b
```{r}
cor(Auto[, sapply(Auto, FUN = (\(x) is.numeric(x)))])
```

### c
```{r}
q9_model = lm(mpg ~. -name, data = Auto)
summary(q9_model)
```

i. Yes
ii. displacement, weight, year, origin
iii. Most recent cars have higher mpg

### d

```{r}
par(mfrow = c(2,2))
plot(q9_model)
par(mfrow = c(1,1))
```

### e

```{r}
summary(lm(mpg ~ cylinders*displacement, data = Auto))
summary(lm(mpg ~ cylinders:displacement, data = Auto))
```

### f

```{r}
par(mfrow=c(2,2))
q9_lm_f = lm(mpg~weight, data= Auto)
plot(q9_lm_f, 1)

q9_lm_f = lm(mpg~log(weight), data= Auto)
plot(q9_lm_f, 1)

q9_lm_f = lm(mpg~sqrt(weight), data= Auto)
plot(q9_lm_f, 1)

q9_lm_f = lm(mpg~poly(weight, 2, raw = T), data= Auto)
plot(q9_lm_f, 1)
par(mfrow=c(1, 1))
```

\newpage
## Q10
### a

```{r}
q10_model = lm(Sales~Price+Urban+US, data = Carseats)
summary(q10_model)
```
### b
```{r}
confint(q10_model, level = 0.95)
```

### h

```{r}
par(mfrow = c(2,2))
plot(q10_model)
par(mfrow = c(1,1))
```

\newpage
## Q11

### a

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2*x+rnorm(100)
```

```{r}
q11_model = lm(y~x)
```

```{r}
summary(q11_model)
```

```{r}
q11_model2 = lm(y~x+0)
```

```{r}
summary(q11_model2)
```

### d

```{r}
se_beta = function(x, y){
  x_dim = dim(x)
  y_dim = dim(y)
  stopifnot(x_dim == y_dim)
  data = data.frame(x = x, y = y)
  n = dim(data)[1]
  #
  beta = sum(data$x * data$y)/ sum(data$x^2)
  #
  SE_beta = sqrt(
    sum(
      (data$y - data$x*beta)^2)/(
        (n-1)*sum(data$x^2))
  )
  #
  beta_ci = c(beta_1 - 2*SE_beta_1, beta_1 + 2*SE_beta_1)
  #
  t_beta = (beta_1 - 0)/SE_beta_1
  #
  return(
    list(
      n = n, beta = beta,
      SE_beta = SE_beta,
      beta_ci = beta_ci,
      t_beta = t_beta)
  )
  #
}
```

```{r}
se_beta(x = x, y = y)
se_beta(x = y, y = x)
```

### f

They are the same

```{r}
summary(lm(y~x+0))
summary(lm(x~y+0))
```

\newpage
## Q12

### a

```{r}
set.seed(1)
x <- rnorm(100)
y <- rnorm(100)
```

### b

```{r}
summary(lm(y~x+0))
summary(lm(x~y+0))
```

\newpage
## Q13

### a, b, c

```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, 0,0.25)
y = -1 + 0.5*x + eps
length(y)
```

```{r}
summary(lm(y~x))
```

### d

```{r}
plot(y~x)
```

### e, f

```{r}
model_xy = lm(y~x)
plot(y~x)
abline(model_xy)
coe_ff <- round(model_xy$coefficients , 2)
text(
  -1.5, -0.4 , 
  paste(
    "Model : ",coe_ff[1] , " + " , 
    coe_ff[2] , "*x"  , "\n\n" , 
    "P-value adjusted = ", 
    round(summary(model_xy)$adj.r.squared,2)),
  cex = 0.5)
```

### g

```{r}
plot(y ~ x, main = "Regression: x ~ y")
for (i in 1:5){
  poly_model = lm(y ~ poly(x, i, raw=TRUE))
  print(paste("Degree: ", i, ", R Squared: ",summary(poly_model)$r.squared))
  # https://r-graph-gallery.com/44-polynomial-curve-fitting.html
  poly_predict <- predict( poly_model ) 
  ix <- sort(x,index.return=T)$ix
  lines(x[ix], poly_predict[ix], col=i, lwd=2 ) 
}
#
legend(
  'topleft', legend = paste0("Degree: ", 1:5),
  col = 1:5, lty=1,lwd=2, cex = 0.5)

```

### h


\newpage
## Q14

### a

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

```{r}
cor(x1, x2)
```
### c

```{r}
summary(lm(y~x1+x2))
```

### d

```{r}
summary(lm(y~x1))
```

### e

```{r}
summary(lm(y~x2))
```

### f

`SE's` halved for both, the p-value was more significant

### g

```{r}
x1 = c(x1, 0.1)
x2 = c(x2, 0.8)
y = c(y, 6)
```


```{r}
summary(lm(y~x1+x2))
```

```{r}
summary(lm(y~x1))
```

```{r}
summary(lm(y~x2))
```

\newpage
## Q15

### a

```{r}
predictor_vars = names(Boston)[-1]
response_vars = names(Boston)[1]
#
for (var in predictor_vars){
  print(paste('Model: ', response_vars, ' ~ ', var))
  print(
    summary(lm(Boston[,response_vars] ~ Boston[,var]))
  )
}
#
```



### b

```{r}
summary(lm(crim~., data = Boston))
```


### a

```{r}
predictor_vars = names(Boston)[-1]
response_vars = names(Boston)[1]
#
for (var in predictor_vars){
  print(paste('Model: ', response_vars, ' ~ ', var))
  print(
    summary(lm(Boston[,response_vars] ~ poly(Boston[,var], 3, raw = T)))
  )
}
#
```






